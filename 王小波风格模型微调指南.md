# 王小波风格模型微调指南

这份指南将引导您完成从原始电子书到微调大型语言模型的完整流程，使其具有王小波的语言风格特征。

## 目录

1. [准备工作](#准备工作)
2. [处理流程概述](#处理流程概述)
3. [Google Colab环境设置](#google-colab环境设置)
4. [数据处理流程](#数据处理流程)
5. [模型微调](#模型微调)
6. [评估与使用](#评估与使用)
7. [常见问题](#常见问题)

## 准备工作

### 所需材料
- 王小波电子书（EPUB格式）
- Google账号（用于访问Google Colab和Google Drive）
- 本项目提供的处理脚本

### 依赖库
所需的Python库包括：
- ebooklib
- beautifulsoup4
- jieba
- tqdm
- pandas
- scikit-learn
- numpy

## 处理流程概述

从王小波电子书到微调模型的完整流程包括以下步骤：

1. **数据提取**：从EPUB格式的电子书中提取纯文本内容
2. **数据清洗**：去除重复内容、噪声数据，保留有价值的文本
3. **数据格式化**：将文本转换为适合大模型训练的格式（问答对、指令-响应格式）
4. **模型微调**：使用LLaMA Factory对大型语言模型进行LoRA微调
5. **模型评估**：评估微调后模型的效果
6. **模型导出**：导出微调后的模型以供使用

## Google Colab环境设置

### 1. 设置Google Drive

```python
# 挂载Google Drive
from google.colab import drive
drive.mount('/content/drive/')

# 创建工作目录
!mkdir -p "/content/drive/My Drive/Colab Notebooks/myLlama"

# 切换到工作目录
%cd "/content/drive/My Drive/Colab Notebooks/myLlama"

# 确认当前工作目录
!pwd
!ls
```

### 2. 克隆项目代码

```python
# 克隆项目代码或上传本地代码
!git clone https://your-repository-url.git
# 或者通过Colab界面上传scripts目录下的所有脚本

# 创建必要的目录结构
!mkdir -p data/raw data/processed models
```

### 3. 安装依赖

```python
# 安装所需依赖
!pip install ebooklib beautifulsoup4 jieba tqdm pandas scikit-learn numpy
```

## 数据处理流程

### 1. 上传电子书

将王小波的EPUB格式电子书上传到Google Drive，可以通过以下方式：

```python
# 方式1：通过Colab界面上传
from google.colab import files
uploaded = files.upload()  # 通过弹出的对话框选择文件

# 方式2：如果电子书已在Google Drive中，可以直接指定路径
epub_path = "/content/drive/My Drive/Colab Notebooks/myLlama/data/raw/wang_xiaobo.epub"
```

### 2. 运行处理流水线

使用`process_pipeline.py`脚本处理电子书，这个脚本会依次调用：
- `epub_to_text.py`：提取文本
- `text_cleaner.py`：清洗与去重
- `format_to_training.py`：生成训练数据

```python
# 确保脚本有执行权限
!chmod +x scripts/process_pipeline.py
!chmod +x scripts/epub_to_text.py
!chmod +x scripts/text_cleaner.py
!chmod +x scripts/format_to_training.py

# 运行处理流水线
!python scripts/process_pipeline.py data/raw/wang_xiaobo.epub -o data/processed
```

您可以通过各种参数自定义处理过程：

```python
# 示例：使用特定参数运行流水线
!python scripts/process_pipeline.py data/raw/wang_xiaobo.epub \
    -o data/processed \
    --similarity-threshold 0.8 \
    --format-type mixed \
    --num-samples 1000
```

### 3. 准备数据分割

将生成的训练数据分为训练集和验证集：

```python
import json
import random

# 读取生成的训练数据
with open("data/processed/03_training/combined_training.json", "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# 随机打乱数据
random.shuffle(data)

# 按8:2分割训练集和验证集
split_idx = int(len(data) * 0.8)
train_data = data[:split_idx]
valid_data = data[split_idx:]

# 保存分割后的数据
with open("data/processed/train.json", "w", encoding="utf-8") as f:
    for item in train_data:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

with open("data/processed/valid.json", "w", encoding="utf-8") as f:
    for item in valid_data:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"训练集：{len(train_data)}条样本")
print(f"验证集：{len(valid_data)}条样本")
```

## 模型微调

使用LLaMA Factory进行模型微调。以下是完整的步骤：

### 1. 克隆LLaMA Factory

```python
# 克隆LLaMA Factory
!git clone https://github.com/hiyouga/LLaMA-Factory.git
%cd LLaMA-Factory
!pip install -e .
```

### 2. 选择基础模型

选择一个合适的中文基础模型，如Chinese-Alpaca-2-7B：

```python
model_name = "ziqingyang/chinese-alpaca-2-7b"  # 基础模型
train_data_path = "/content/drive/My Drive/Colab Notebooks/myLlama/data/processed/train.json"
valid_data_path = "/content/drive/My Drive/Colab Notebooks/myLlama/data/processed/valid.json"
output_dir = "/content/drive/My Drive/Colab Notebooks/myLlama/models/wang_xiaobo_style"
```

### 3. 开始微调

使用LoRA方法进行微调：

```python
!python src/train_bash.py \
    --model_name_or_path $model_name \
    --do_train \
    --dataset $train_data_path \
    --template alpaca \
    --finetuning_type lora \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --output_dir $output_dir \
    --overwrite_cache \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 500 \
    --eval_steps 500 \
    --evaluation_strategy steps \
    --eval_dataset $valid_data_path \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --plot_loss \
    --fp16
```

微调参数说明：
- `--lora_rank`：LoRA的秩，值越大模型可塑性越高
- `--per_device_train_batch_size`：每个设备的批次大小
- `--gradient_accumulation_steps`：梯度累积步数
- `--learning_rate`：学习率
- `--num_train_epochs`：训练轮数

## 评估与使用

### 1. 微调后的模型评估

```python
# 使用验证集评估模型
!python src/evaluate.py \
    --model_name_or_path $model_name \
    --finetuning_type lora \
    --checkpoint_dir $output_dir \
    --dataset $valid_data_path \
    --template alpaca
```

### 2. 交互式测试

通过交互式界面测试模型效果：

```python
# 运行交互式演示
!python src/cli_demo.py \
    --model_name_or_path $model_name \
    --template alpaca \
    --finetuning_type lora \
    --checkpoint_dir $output_dir
```

示例提示词测试：
- "请用王小波的风格描述当代生活"
- "以王小波的口吻评价现代科技"
- "如果王小波活在今天，他会如何看待社交媒体？"

### 3. 导出模型

```python
# 将LoRA权重合并到基础模型中（可选）
!python src/export_model.py \
    --model_name_or_path $model_name \
    --template alpaca \
    --finetuning_type lora \
    --checkpoint_dir $output_dir \
    --export_dir $output_dir/merged_model
```

## 常见问题

### 1. 如何提高模型对王小波风格的把握？

- **增加训练数据**：收集更多王小波的作品
- **优化数据质量**：确保清洗后的数据保留了王小波风格的特点
- **调整指令模板**：在`format_to_training.py`中添加更多与王小波风格相关的指令模板
- **风格特征提取**：增强`analyze_paragraph_style`函数的特征识别能力

### 2. 内存或显存不足怎么办？

- 减小`per_device_train_batch_size`
- 增加`gradient_accumulation_steps`
- 使用`--bf16`或`--fp16`启用混合精度训练
- 使用更小的基础模型，如1.3B或3B参数量的模型

### 3. 如何评估模型风格模仿的效果？

- **盲测比较**：让人类评估者无差别比较模型输出和王小波原文
- **特征分析**：分析生成文本中的句长、常用词、修辞手法等特征是否符合王小波风格
- **相似度评分**：使用文本嵌入模型计算生成文本与王小波作品的风格相似度

### 4. 数据处理时遇到错误怎么办？

- 检查EPUB文件格式是否正确
- 尝试调整清洗参数，如`similarity_threshold`
- 分步运行流水线，找出问题所在的具体步骤
- 查看日志，了解详细错误信息

---

通过本指南的步骤，您应该能够成功从王小波的电子书中提取特征，并微调出具有他独特文风的语言模型。祝您创作愉快！

*注：微调得到的模型应仅用于学术研究和个人学习，请尊重知识产权和作者声誉。* 