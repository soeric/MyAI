# LLM微调项目使用指南

本指南将帮助您使用LLaMA Factory在Google Colab上对大型语言模型进行微调。

## 目录

1. [项目概述](#项目概述)
2. [环境准备](#环境准备)
3. [数据准备](#数据准备)
4. [模型微调](#模型微调)
5. [模型评估与导出](#模型评估与导出)
6. [常见问题](#常见问题)

## 项目概述

本项目基于[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)，这是一个用于微调大型语言模型的开源工具箱。LLaMA Factory支持多种模型（如LLaMA、Mistral、Baichuan、ChatGLM等）以及多种微调方法（如LoRA、QLoRA、全参数微调等）。

本项目的主要特点：
- 使用Google Colab的GPU资源进行模型微调
- 支持中文和英文模型的微调
- 提供数据处理脚本，方便数据格式转换
- 提供完整的微调流程示例

## 环境准备

1. 打开Google Colab：[https://colab.research.google.com/](https://colab.research.google.com/)
2. 创建新笔记本或上传本项目中的`llama_factory_colab.py`（需要转换为Jupyter笔记本格式）
3. 在Colab中设置GPU运行时：
   - 点击菜单`修改 > 笔记本设置`
   - 在硬件加速器下拉菜单中选择`GPU`
   - 点击保存

## 数据准备

### 数据格式

LLaMA Factory支持多种数据格式，本项目主要使用Alpaca格式：
```json
{
  "instruction": "写一首关于春天的诗",
  "input": "",
  "output": "春风轻拂面，花开满园香。鸟儿歌唱早，春意正悠扬。"
}
```

### 数据处理

1. 将原始数据转换为Alpaca格式：
   ```bash
   # 在本地运行
   python scripts/prepare_data.py --input your_data.csv --output data/processed_data.json --split
   ```
   
   或者在Colab中使用：
   ```python
   # 上传原始数据
   from google.colab import files
   uploaded = files.upload()  # 上传您的数据文件
   
   # 安装依赖
   !pip install pandas tqdm
   
   # 运行数据处理脚本
   !python scripts/prepare_data.py --input your_data.csv --output data/processed_data.json --split
   ```

2. 将处理好的数据放入`data/`目录

## 模型微调

本项目使用LoRA方法进行微调，这是一种参数高效的微调方法，可以在有限的计算资源上微调大型语言模型。

### 微调示例

完整的微调命令如下（在Colab笔记本中执行）：

```python
# 克隆LLaMA Factory
!git clone https://github.com/hiyouga/LLaMA-Factory.git
%cd LLaMA-Factory
!pip install -e .

# 微调模型（以Chinese-Alpaca-2-7B为例）
model_name = "ziqingyang/chinese-alpaca-2-7b"  # 基础模型
dataset_path = "data/processed_data_train.json"  # 训练数据
eval_dataset_path = "data/processed_data_val.json"  # 验证数据
output_dir = "output"  # 输出目录

!python src/train_bash.py \
    --model_name_or_path $model_name \
    --do_train \
    --dataset $dataset_path \
    --template alpaca \
    --finetuning_type lora \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --output_dir $output_dir \
    --overwrite_cache \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --plot_loss \
    --fp16
```

### 常用微调参数说明

- `--model_name_or_path`: 基础模型路径或Hugging Face上的模型ID
- `--dataset`: 训练数据集路径
- `--template`: 数据模板，如alpaca, sharegpt等
- `--finetuning_type`: 微调方法，如lora, qlora, full等
- `--lora_rank`: LoRA的秩
- `--lora_alpha`: LoRA的alpha参数
- `--per_device_train_batch_size`: 每个设备的批次大小
- `--learning_rate`: 学习率
- `--num_train_epochs`: 训练轮数
- `--fp16`: 使用半精度浮点数训练，节省显存

## 模型评估与导出

### 模型评估

```python
# 使用验证数据评估模型
!python src/evaluate.py \
    --model_name_or_path $model_name \
    --finetuning_type lora \
    --checkpoint_dir $output_dir \
    --dataset $eval_dataset_path \
    --template alpaca
```

### 测试微调后的模型

```python
# 运行交互式演示
!python src/cli_demo.py \
    --model_name_or_path $model_name \
    --template alpaca \
    --finetuning_type lora \
    --checkpoint_dir $output_dir
```

### 导出模型

```python
# 将LoRA权重合并到基础模型中
!python src/export_model.py \
    --model_name_or_path $model_name \
    --template alpaca \
    --finetuning_type lora \
    --checkpoint_dir $output_dir \
    --export_dir merged_model
```

### 保存到Google Drive

```python
# 将模型保存到Google Drive
from google.colab import drive
drive.mount('/content/drive')

save_path = "/content/drive/MyDrive/llm_models/finetuned_model"
!mkdir -p $save_path
!cp -r $output_dir/* $save_path/
```

## 常见问题

### 显存不足

如果遇到显存不足的问题，可以尝试：
- 减小`per_device_train_batch_size`
- 增加`gradient_accumulation_steps`
- 使用`--bf16`或`--fp16`启用混合精度训练
- 使用`--deepspeed`配置DeepSpeed加速

### 训练中断

Colab免费版有使用时间限制，如果训练中断：
- 定期保存检查点（设置较小的`save_steps`）
- 使用`--resume_from_checkpoint`从检查点恢复训练
- 考虑使用Colab Pro或其他平台进行长时间训练

### 更多资源

- [LLaMA Factory官方文档](https://github.com/hiyouga/LLaMA-Factory#readme)
- [LLaMA Factory参数指南](https://github.com/hiyouga/LLaMA-Factory/blob/main/docs/Parameters.md)
- [HuggingFace平台](https://huggingface.co/)上寻找优质基础模型 